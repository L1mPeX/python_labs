# –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ ‚Ññ 3

## –¢–µ–∫—Å—Ç—ã –∏ —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ (—Å–ª–æ–≤–∞—Ä—å/–º–Ω–æ–∂–µ—Å—Ç–≤–æ)

text.py
```python
import re
from typing import List, Tuple, Dict

def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    if casefold:
        text = text.casefold()
    if yo2e:
        text = text.replace('—ë', '–µ').replace('–Å', '–ï')
    text = re.sub(r'[\t\r\n]+', ' ', text)
    text = re.sub(r' +', ' ', text).strip()
    return text

token_pattern = re.compile(r'\b\w+(?:-\w+)*\b', re.UNICODE)

def tokenize(text: str) -> List[str]:
    return token_pattern.findall(text)

def count_freq(tokens: List[str]) -> Dict[str, int]:
    freq = {}
    for token in tokens:
        freq[token] = freq.get(token, 0) + 1
    return freq

def top_n(freq: Dict[str, int], n: int = 5) -> List[Tuple[str, int]]:
    return sorted(freq.items(), key=lambda x: (-x[1], x[0]))[:n]
```

text_stats.py
```python
import re
from typing import List, Tuple, Dict

def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    if casefold:
        text = text.casefold()
    if yo2e:
        text = text.replace('—ë', '–µ').replace('–Å', '–ï')
    text = re.sub(r'[\t\r\n]+', ' ', text)
    text = re.sub(r' +', ' ', text).strip()
    return text

token_pattern = re.compile(r'\b\w+(?:-\w+)*\b', re.UNICODE)

def tokenize(text: str) -> List[str]:
    return token_pattern.findall(text)

def count_freq(tokens: List[str]) -> Dict[str, int]:
    freq = {}
    for token in tokens:
        freq[token] = freq.get(token, 0) + 1
    return freq

def top_n(freq: Dict[str, int], n: int = 5) -> List[Tuple[str, int]]:
    return sorted(freq.items(), key=lambda x: (-x[1], x[0]))[:n]
```

–î–ª—è —Ç–µ—Å—Ç–æ–≤ –µ—Å—Ç—å —Å–∫—Ä–∏–ø—Ç tests.py
```python
from lib.text import normalize, tokenize, count_freq, top_n

def test_normalize():
    assert normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t") == "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"
    assert normalize("—ë–∂–∏–∫, –Å–ª–∫–∞") == "–µ–∂–∏–∫, –µ–ª–∫–∞"
    assert normalize("Hello\r\nWorld") == "hello world"
    assert normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ") == "–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"

def test_tokenize():
    assert tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä") == ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]
    assert tokenize("hello,world!!!") == ["hello", "world"]
    assert tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ") == ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]
    assert tokenize("2025 –≥–æ–¥") == ["2025", "–≥–æ–¥"]
    assert tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ") == ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]

def test_count_freq_and_top_n():
    tokens = ["a","b","a","c","b","a"]
    freq = count_freq(tokens)
    assert freq == {"a":3, "b":2, "c":1}
    assert top_n(freq, 2) == [("a",3), ("b",2)]

    tokens2 = ["bb","aa","bb","aa","cc"]
    freq2 = count_freq(tokens2)
    assert top_n(freq2, 2) == [("aa",2), ("bb",2)]

def run_all_tests():
    test_normalize()
    test_tokenize()
    test_count_freq_and_top_n()
    print("–í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")

if __name__ == "__main__":
    run_all_tests()
```

–°–∫—Ä–∏–Ω—à–æ—Ç –≤—ã–≤–æ–¥–∞ —Ç–µ—Å—Ç–æ–≤:
![–°–∫—Ä–∏–Ω—à–æ—Ç 1](./materials/imgage.png)

## –õ–∏—Ü–µ–Ω–∑–∏—è <a name="license"></a>

[![License: CC BY-NC-SA 4.0](https://licensebuttons.net/l/by-nc-sa/4.0/80x15.png)](https://creativecommons.org/licenses/by-nc-sa/4.0/)
–ü—Ä–æ–µ–∫—Ç –¥–æ—Å—Ç—É–ø–µ–Ω —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º –Ω–∞ —É—Å–ª–æ–≤–∏—è—Ö [–õ–∏—Ü–µ–Ω–∑–∏–∏ CC BY-NC-SA 4.0](./LICENSE).

_–ê–≤—Ç–æ—Ä—Å–∫–∏–µ –ø—Ä–∞–≤–∞ 2025 –ê–Ω–¥—Ä–µ–π –ö–∞–∑–∞—Ä–∏–Ω_
